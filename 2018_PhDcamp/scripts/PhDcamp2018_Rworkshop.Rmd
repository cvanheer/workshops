---
title: "PhDcamp2018_Rworkshop"
output: pdf_document 
---
Welcome to the PhD camp for 2018! 

This is an R Markdown script that we are going to run parts of. We are going to go through some essentials for learning to use R if you are a beginner. RStudio (the program you are in at the moment) makes R much easier to run. There are also some alternatives:

## Jamovi
Jamovi is an interface for running R but with a nice, user-friendly point-and-click interface. The advantage of using it over SPSS is it gives you the underlying code for your analyses. 
https://www.jamovi.org 

## RKWard
RKWard is another user friendly interface for editing R, with a specific focus on statistics heavy users. It is good for beginners, it allows you to edit plots, and data in a spreadsheet like interface. (Thanks to Kevin Shabahang who pointed this out on the GRIPS facebook page!)
https://rkward.kde.org

## Jupyter
Jupyter is an interactive envrionment where you can setup your data analysis so that you can share it with your colleagues and others who may read your papers. It provides a transparent interface in which to do your data analysis and allows others to reproduce the same data analysis. I often see it used for authors who want to post their data analysis online. Like RNotebook documents, it allows you to play sections of your script. Jupyter allows you to code in a number of languages (over 40). Some of the common ones are python, R, Julia
http://jupyter.org 
https://www.datacamp.com/community/blog/jupyter-notebook-r (gives you more info about the R specifics)

```{r settings}

# We can make comments that R doesn't read using the '#' sign 
# ----------------------------------------------------------------------------
#                        Workspace/Knitting settings
# ----------------------------------------------------------------------------
knitr::opts_chunk$set(echo = TRUE) # these are options for knitting the document
rm(list = ls(all = TRUE)) # delete all variables
```


```{r}
# This creates a new section of code

# OR - we can use a keyboard shortcut to do this 

# Try typing: Cmd and Option and I - on your keyboard 
```

```{r calculator}

# I can add comments using the '#' number sign! 

# I can use R just like I would a normal calculator 
# ------- Adding -------------
1 + 1
# ------- Multiplying --------
10 * 100
# ------- Subtracting --------
100 - 50
# ------- Dividing -----------
100 / 50 

# ------ Multiple operations -----
(100 - 50) + 10 * 5 # will follow BODMAS (brackets) then runs multiplication, then addition
```


Before we do anything else, we are going to learn about variables. You can think of a variable as a container that we have labelled, and holds stuff
We need variables because we need to pull out data out and store it in things, and we also need to store other information, like the mean, the standard deviation,and the results of statistical tests somewhere
```{r learn_about_variables}

# In R, most of the time we use <- to assign something. Other languages uses the "=" sign, but when we put something in a container, we usually use
# I'll go over the exceptions later (they are few, and far between!)
x <- 6
y <- 1

# Task: type x and then press 'Enter' in the console 
# Task: change the value of x to 5
# Task: Add x and y, and assign it to a variable called z
z <- x + y

# print(variable) will print stuff to the screen 
# You can see that z is just the sum of x and y!
print(z)

# We don't have to limit ourselves to one or two 

# Concatenating variables
z <- c(1,2,3)


# Vectors
vector(mode="logical", length=4)

# Lists
participant.info <- list("firstName" = "Jimbob Ghostkeeper", 
                         "age" = 29, 
                         "dob" = "09.12.1988",
                         "suburb" = "Chelsea",
                         "physiological_age" = 50,
                         "educationInYears" = 21,
                         "cars" = list("toyota", "honda"),
                         "childrenAge" = c(4,7,15))

# You can have a list which contains another list
# You can have a list which contains a vector of numbers too!

# ----------------------------------------------------
#                   Tasks
# ----------------------------------------------------
# 1. create a list called 'labHeads' with the following 
# fields / names: 
# "names" = a list of names - "rob", "olivia", "katherine", "patrick", "jason", "philip", "hinze", "bob"
# "ages" = a bunch of numbers (be nice)
# 'title' = e.g. "prof", "dr", "assProf"


# Creating data frames that are combinations of factors (expand.grid)


```

Let's try and setup where the workshop files are on your computer. You should have all of this in a folder called 'PhDCamp_RWorkshop_2018'
First things first: let's figure out what your directory format looks like. Sometimes the way we specify directories in R and 

```{r setup_computer}

# ----------------- Setup base folder ---------------------------
exp.basedir <- "/Users/Chris/Bitbucket/2018_PhDcamp/scripts" # base folder (you can change this, everything else is relative)
exp.rawdir <- file.path(exp.basedir, "data") # raw data files only
#exp.procdir <- file.path(exp.basedir, "processed_data")  # processed raw files
```

Whenever we start a script, we can add some useful settings. We can store information in R as 'variables' which are like books in a library. When we want information about a specific book, we pull it out, and when we want information about a variable in R, we just type it's name in the console, and get what's inside the book. 

I usually start by deleting all of the variables. This is kind of like burning all of the books in the library and getting rid of them, which is a bad analogy, but here I am, using it!



```{r load_packages}

# ----------------------------------------------------------------------------
#                          Load / install relevant packages
# ----------------------------------------------------------------------------
# Packages are just "folders" we download from the R website and use to do fancy stuff with R
# The beauty of R is that anyone can make a package (bunch of scripts) and post it online
# R is open source (you dont have to pay for it) and user driven unlike SPSS so you dont have to wait 
# around until something you want appears, it is usually there, or you can just make it yourself! 

# Although R comes with lots of packages, we don't want R to use all of them - loading these takes memory!
# Instead, we only load what we need
# When we run the library command below, we are telling R to load these packages so we can use everything in them
library("plyr", "ggplot2") # We can separate packages with commas 
```


```{r}
# ----------------------------------------------------------------------------
#                    Task:       Setup directories
# ----------------------------------------------------------------------------
# Now we can read the csv file - header=TRUE means that the read.csv function will take the column 
# headings from your csv file and then use them for the names of your data frame 
file_to_read <- "raw_data.csv"
raw_data <- read.csv(file.path(exp.rawdir, file_to_read), header = TRUE, sep = ",")
data <- raw_data

# What if I want to change some column names, I hear you ask!! 
# I have commented this out because our script uses the names from the excel file
# you could do this names(data)[1] <- "col1" - this names the first column of data to "col1" and so on
# you could also do colnames(data) <- c("subj_no" = "sub_no", "stim" = "stimuli") - so we tell R that we 
# want to change "subj_no" to "sub_no" and so on - this way we dont have to remember which column the 
# thing we want is 

# Let's have a quick look at our data se we can see the column names etc 
# -------------- Task -----------------------------
# Reading column and row names 

data$conf # reads out the confidence column

data$conf[data$subj_no==1] # reads out confidence values only for subject 1

data$conf[data$subj_no==1 & data$subj_no==2] # reading out confidence values for subject 1 AND 2

data[data$subj_no==1, ] # read out all rows where subject number is 1, and ALL columns for this subject
# remember the format we use is [rows, columns] 

# We can also read out stuff this way:
data[,'conf'] # read out the confidence column

# but this is generally a bit cumbersome 

```

We do not need every column of the dataset. So we can take only the columns we want out of raw data, and store these as a new data frame called 'data'
```{r create_data}

# Create a new dataset with nicely named variables and only what we need
# ----------------------------------------------------
#                   Tasks
# ----------------------------------------------------
# 1. Ohno! We forgot the subject number! Add something to the 
# code above so we can include subject number
# Hint: type  head(raw_data) and have a look at the names of the 
# first few columns so you know what to put in there 

# We can also store *both* of these datasets in a list
allData <- list("raw_data" = raw_data, 
                "data" = data)

allData <- list("raw_data" = raw_data,
     "my_data"= data)

# We can access the labels in these data in two ways:
# 1. Using the label name we setup just before:
allData["raw_data"] 
allData["data"] 

# 2. Using the order in which we made the list: remember that "raw_data" was the fist label
allData[1]
# and "data" was the second label
allData[2]

```



```{r explore_data}
# ----------------------------------------------------
#                   Tasks
# ----------------------------------------------------
# Task: get the column names of the new dataset 
# Task: get the rownames of the new dataset
# Task: get the head (first few rows) of the new dataset


# Let's look at each column and row a bit more
# We have constructed the data into a data frame, which you can think of as being very similar to an excel spreadsheet
# We can look at the whole dataset by typing the following in the console:
data

# If we want to look at individual columns, we can look at it using two methods:
data$group
data['group'] 

# Let's explore the data further by calculating the mean
# and the standard deviation 
data[data$subj_no==1, 'RT_day1']

mean(data[data$subj_no==1, 'RT_day1'], na.rm = TRUE)
sd(data[data$subj_no==1, 'RT_day1'], na.rm = TRUE)

# We will learn how to do this for all subjects later 
```


Often we need to learn how to get specific columns and rows of data 
For example, we might want just the ages of subjects, or only subjects over 40 etc. 
There are a number of ways you can do this in R, it's good to know some of them
One thing that you will notice in R, is that the same thing can be done a few different ways 
A lot of the actual work you will do is figure out how to filter and subset your data before you do the actual data analysis 
```{r subsetting_data}
library("plyr") # we are going to use a package called pylr 
library("dplyr")
# ----------------------------------------------------
# Ex. Splitting data by gender 
# ----------------------------------------------------
# Filter = Filter a data frame by a specifc value, return all columns that
# Select = Select a column from the resulting data frame 
filterData <- filter(data, group==2)
dataBygroup <- select(filterData, RT_day1)

# ---------------------------------------------------------------
# Ex. Splitting data by Age using subset, or select + filter 
# --------------------------------------------------------------
dataByAge1 <- subset.data.frame(data, age>25, select=c(RT_day1, RT_day2))
dataByAge2 <- select(filter(data, age>25), c(RT_day1, RT_day2)) # you could do the same thing using 'select' 

# We can prove subset and select + filter do the same thing  by using "=="


# -------------------------------------------------------------
# Ex. Splitting data using just data frames (no fancy stuff)
# -------------------------------------------------------------

# Say I want Av.SSRT, RT.go, RT.go.sd for people who are older than 20

data$age>20 # tells us which people are older than 20 

# Now that we know this gives us true or false values, we put those true and false values 
# in the rows of the data frame - R will select only the rows where this is equal to 'TRUE' 
# i.e. the columns where that person's age is above 20 
data[data$age > 20, ] # gives us all of the data for a single person that is older than 20

data[data$age >20, c("group","RT_day1")] # gives us specific columns 

# -------- Handling Missing Values ---------------
# We can use na.omit (type ?na.omit for help in the console)
na.omit( data[data$age >20 & data$age <40, c("RT_day1", "RT_day2")] ) # gives us specific columns 


# We are not just limited to one condition either: we can get two conditions
# Here we get data where people are aged between 25+ and less than 40 
dataByAge3 <- select(filter(data, age>25 & age<40), RT_day1) # you could do the same thing using 'select' 

```


Sometimes you might also want to get a quick table with descriptive stats, summarising things by group
```{r get_descriptive_stats}


# ----------- Using summary functiion ----------------

# 'summary' can give us some useful stats 


# ----------- Using the psych package ----------------
library("psych")
descriptives <- describeBy(data, group=data$group) # descriptive stats 

# ----------- Using aggregate -----------------------
# Aggregate takes a formula, which works in this order:
# formula = thing_todo_stats_on ~ grouping variables sep by '+'
# below, I am doing stats on RT_day1 and I am
# grouping or separating by group and difficulty
# Aggregate also takes the thing you want to use, i.e. the data frame
# and you also need to include a function - i.e. what you want to do with
# the data, e.g. calculate the mean 
aggData <- aggregate(formula = RT_day1 ~ difficulty + group, 
                     data = data, 
                     FUN = mean)

# ----------- Using arrange -----------------s------
# Arrange gets your data frame and organises it by 
# a specific variable 
arrangedData <- arrange(data, data$subj_no)
arrangedData <- arrange(data, data$group) 

# ----------------------------------------------------
#                   Tasks
# ----------------------------------------------------
# 1. Try getting the summary of 'data' - summary takes an argument 
# of the data frame you want stats on
summary(data$RT_day1)

# 2. Aggregate data by Av.SSRT for each 'ageBand' and 'group'
# and take the sd 




# ---------- Using pipes ----------------------------
# Pipes are signified by '%>%' - they allow us to take 
# the data, do stuff with it, and then 'pass' it to another
# step without having to store the result of each step as 
# an 'intermediate' variable

# This pipe takes data, gets only the rows where gender is 1
# it then gets rid of any rows with 'NA' / missing values
# takes the result of that step, and gets a column labelled 
# Av.SSRT for *only* rows which passed the other earlier steps 

temp <- data %>% # handball the data to the next step - filter by gender 
  filter(!is.na(group == 1)) %>% # handball the group==1 rows to the next step
  select(RT_day1) %>%
  na.omit(RT_day1) 
  

# Get the rows where gender==1 and get values for each row, from column labelled 'Av.SSRT' 
# The final result after it's been handballed everywhere is sent to 'temp' 

# ----------------------------------------------------
#                   Tasks
# ----------------------------------------------------
# 1. Run the same as above *but*
# - take data
# - filter for ages > 20
# select RT.go 
# take the mean of RT.go after you are done 

library("tidyr")
temp <- data %>%
  na.omit(select(group, RT_day1,  RT_day1)) %>%
  group_by(group) %>%
  summarise(RT = mean(RT_day1), mean(RT_day1))
```


```{r subset_by_subject}

# -----------------------------------------------
#                 Task
# -----------------------------------------------
# 1. Subset the data (any way you like) by subject and 
# get the mean of go and nogo RT's for each subject in a table
subj_aggData1 <- aggregate(formula = RT_day1 ~ subj_no, 
          data = data, 
          FUN = mean)
subj_aggData1

# What if we want this for both RT_day1 and RT_day2 
# There are easier ways to do this 
subj_aggData2 <- aggregate(formula = RT_day2 ~ subj_no,
          data = data,
          FUN = mean)
subj_aggData1

# We can put these two into a data frame together 
RT_summary_data <- data.frame(cbind(
  subj_no = subj_aggData1[,1],  # subject no
  subj_no = subj_aggData1[,2], # RT_day1
  subj_aggData1[,2])) #  RT_day2


# But there is another way to do this 
RT_data <- ddply(data, .(subj_no), function(data){
  
  # We now just have only subject data in this container
  
  # We are going to make a new data frame that puts all of the things 
  # we want on it 
  RT_data <- data.frame(
    mean_day1 = mean(data$RT_day1),
    sd_day1 = sd(data$RT_day1),
    mean_RT_day2 = mean(data$RT_day2),
    sd_day2 = sd(data$RT_day2))
  
  return(RT_data)})

# Let's create some histograms
hist(data$RT_day1)
hist(data$RT_day2)

# What if we want these on the same panel? 
par(mfrow=c(1,2))
hist(data$RT_day1)
hist(data$RT_day2)

# Let's attach a boxplot to a scatterplot 
par(fig=c(0,0.8,0,0.8), new=TRUE)
plot(data$RT_day1,data$RT_day2, xlab="RT",
  ylab="RT (s)")
par(fig=c(0,0.8,0.55,1), new=TRUE)
boxplot(data$RT_day1, horizontal=TRUE, axes=FALSE)
par(fig=c(0.65,1,0,0.8),new=TRUE)
boxplot(data$RT_day1, axes=FALSE)
mtext("Scatterplot", side=3, outer=TRUE, line=-3)


# We could also crate a boxplot which looks cleaner 
boxplot( x = data$RT_day1,           # the data
         xlab = "RT_day1",  # x-axis label
         ylab = "",
         border = "grey50",
         frame.plot = FALSE,
         staplewex = 0,
         whisklty = 1, # solid line for whisker 
         horizontal = TRUE) 

```


Statistical tests - here we are going to run a t-test, ANOVA and regression 

```{r check_distributions}


# Check the distribution for each of the groups
hist(data$RT_day1)
hist(data$RT_day2)

library("stats")
qqnorm(y=data$RT_day1)
qqnorm(y=data$age)

# Group RT_day1 and RT_day2 by group and take the mean
data.barplot.mean <- aggregate(formula = RT_day1 + RT_day2 ~ group, 
                     data = data, 
                     FUN = mean)

barplot(height=table(data.barplot.mean))

# install.packages("gplots") # uncomment this 
library("gplots")
plotmeans(formula = RT_day1 ~ group, 
          data = data,
          xlab = "Group 1 = Sz, 2 = Control",
          ylab = "RT_day1",
          n.label = TRUE) # don't display sample size
```

```{r run_anova_ttest}
# install.packages("broom") # uncomment this 
# Perform t-test
library('broom')

# Run a t-test - the default is to run a t-test that assumes unequal variances (Welsh)
test_output <- t.test(formula = data$RT_day1 ~ data$group, data = data)
# Task: run 'tidy' on output of t-test
tidy_test_ouput <- tidy(test_output)

# Run a student's t-test - add var.equal = TRUE
t.test(formula = RT_day2 ~ group, data = data,
var.equal = TRUE)

# Run an ANOVA
anova_test <- aov(RT_day2 ~ group + difficulty, data = data)
summary(anova_test)
tidy(anova_test)

```

Correlations
```{r}

plot(data$RT_day1, data$RT_day2)
cor(data$RT_day1, data$RT_day2, use="complete.obs", method="pearson")

```

Regression: to run a regression, you need to place a formula in 
```{r regression}

# install.packages("broom")
library('broom') # allows you to use 'tidy' functions which clean up messy R output

# Create a regression model 
# When R runs the regression model, it creates a list of all of the sections of outputatt
model1 <- lm(data$RT_day2 ~ data$group +data$RT_day1)

# lm.beta

# Let's have a look at what the sections of the list there are from running the regression model 
attributes(model1)

# --- Plot residuals -----
hist(model1$residuals)

# --- Get some neat output ----
tidy(model1)

# --- Plot predictions from the model ----
plot(model1$fitted.values)

# -------
```


